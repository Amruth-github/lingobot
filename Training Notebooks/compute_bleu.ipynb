{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./test.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_data = random.sample(test_data, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./results/v5/model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/v5/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    max_length = 200  # Adjust as needed\n",
    "    generated = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1)\n",
    "    corrected_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [i['input_text'] for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = [correct(i['input_text']) for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_linguistic_errors(original, corrected):\n",
    "    original_tokens = nltk.word_tokenize(original)\n",
    "    corrected_tokens = nltk.word_tokenize(corrected)\n",
    "    distance = edit_distance(original_tokens, corrected_tokens)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ler_for_corpus(original_corpus, corrected_corpus):\n",
    "    total_errors = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for original, corrected in zip(original_corpus, corrected_corpus):\n",
    "        error_count = count_linguistic_errors(original, corrected)\n",
    "        total_errors += error_count\n",
    "        total_tokens += len(nltk.word_tokenize(original))\n",
    "\n",
    "    ler = total_errors / total_tokens\n",
    "    return ler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ler = calculate_ler_for_corpus(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For v4 : 0.0590315405235841\n"
     ]
    }
   ],
   "source": [
    "print(f\"For v4 : {corpus_ler}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ler_v5 = calculate_ler_for_corpus(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For v5 : 0.05171591234375497\n"
     ]
    }
   ],
   "source": [
    "print(f\"For v5 : {corpus_ler_v5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import corpus_gleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735216837295136"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_gleu([[i.split()] for i in reference], [i.split() for i in candidate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388339313827006"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([[i.split()] for i in reference], [i.split() for i in candidate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8766\n",
      "Recall: 0.8137\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_recall(reference, candidate):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for ref_sent, cand_sent in zip(reference, candidate):\n",
    "        # Convert the lists of tokens to sets for efficient comparison\n",
    "        ref_set = set(ref_sent)\n",
    "        cand_set = set(cand_sent)\n",
    "\n",
    "        # Calculate true positives\n",
    "        true_positives += len(ref_set.intersection(cand_set))\n",
    "\n",
    "        # Calculate false positives\n",
    "        false_positives += len(cand_set - ref_set)\n",
    "\n",
    "        # Calculate false negatives\n",
    "        false_negatives += len(ref_set - cand_set)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0.0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "precision, recall = calculate_precision_recall([i.split() for i in reference], [i.split() for i in candidate])\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f0_5_score(precision, recall, beta=0.5):\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_5_score = calculate_f0_5_score(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F0.5 Score = 0.863267962575421\n"
     ]
    }
   ],
   "source": [
    "print(f\"F0.5 Score = {f0_5_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
